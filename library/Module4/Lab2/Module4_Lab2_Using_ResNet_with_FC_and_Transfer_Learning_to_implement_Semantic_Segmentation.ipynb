{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4, Lab 2: Using ResNet with Fully Convolutional Layers and Transfer Learning to Implement Semantic Segmentation\n",
    "\n",
    "In this tutorial, we will add fully-convolutional layers to the ResNet model using transfer learning.\n",
    "\n",
    "We will introduce and use the concept of *Transfer Learning*, where pre-existing learned knowledge is used to inform our approach and improve our results.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "We built up a lot of code in the last tutorial, so we won't replicate it.  Instead, we're going to import Python files from the directory of this lab that implements the same functionality. Feel free to view that code to inspect it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cntk-2.4-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\n",
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Microsoft Cognitive Toolkit version 2.5.1\n",
      "Using numpy version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "#\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root\n",
    "# for full license information.\n",
    "# ==============================================================================\n",
    "\n",
    "# For Azure Notebooks, we will update Microsoft Cognitive Toolkit version to 2.4 \n",
    "# you can comment out the following line if you are running in your own local Jupyter Notebook setup and already have\n",
    "# CNTK 2.4 installed\n",
    "!pip install --upgrade --no-deps https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\n",
    "\n",
    "import cntk as C\n",
    "print (\"Using Microsoft Cognitive Toolkit version {}\".format(C.__version__))\n",
    "\n",
    "import numpy as np\n",
    "print (\"Using numpy version {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "from cntk.learners import learning_rate_schedule, UnitType\n",
    "from cntk.device import try_set_default_device, gpu\n",
    "from tqdm import tqdm\n",
    "from cntk.initializer import he_normal\n",
    "from cntk.layers import AveragePooling, BatchNormalization, Convolution, Dense\n",
    "from cntk.ops import element_times, relu, sigmoid\n",
    "from cntk import load_model, placeholder, Constant\n",
    "\n",
    "import coco   # local class to read the COCO images and labels\n",
    "import helper # some functions to plot images\n",
    "import training_helper  #\n",
    "import cntk_resnet_fcn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths relative to current python file.\n",
    "abs_path  = os.path.dirname(os.path.abspath(\".\"))\n",
    "data_path = os.path.join(abs_path, \"../../data/M4\")\n",
    "zip_path = os.path.join(abs_path, \"../data-zip\")\n",
    "\n",
    "model_path = os.path.join(abs_path, \"Lab2/models\")\n",
    "base_model_file = os.path.join(model_path, \"ResNet18_ImageNet_CNTK.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Cognitive Toolkit's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] The Cognitive Toolkit is using the CPU for processing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    isUsingGPU = C.device.try_set_default_device(C.device.gpu(0))\n",
    "except ValueError:\n",
    "    isUsingGPU = False\n",
    "    C.device.try_set_default_device(C.device.cpu())\n",
    "    \n",
    "print (\"[i] The Cognitive Toolkit is using the {} for processing\".format(\"GPU\" if isUsingGPU else \"CPU\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you are interested in running training (and have a GPU, or a CPU with lots of patience), set `make_model` to true and run the training code below.\n",
    "\n",
    "Otherwise, set this to false, and download the already-baked pre-trained model directly from Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Nowadays, few convolutionl image networks are trained from scratch with purely random initialization.  Mostly, this is because of the difficulty in finding a suitably large, labelled, dataset.  If the target dataset is very small, it is likely that training with it would lead to unacceptably large generalization error.\n",
    "\n",
    "Instead, the most common approach is to *pre-train* using a more generic large dataset (for example, ImageNet, or CoCo) and then use the learned features from this set as an initial seed to continue training with the target smaller dataset.  \n",
    "\n",
    "This is using the intuition that the knowledge gained (within the layers of the network) while learning to recognize certain objects could be useful when trying to recognize objects of a different type. In other words, the network will learn more useful and generic *features* in its layers when trained against a larger dataset, which it can then apply (*transfer*) to the dataset of interest.\n",
    "\n",
    "This approach is called *Transfer Learning*. \n",
    "\n",
    "![Transfer Learning](images/Transfer_Learning.png \"Transfer Learning\")\n",
    "\n",
    "## Considerations when using Transfer Learning\n",
    "\n",
    "If the target dataset is sigificantly smaller in size but of similar content to the original dataset, it is not always a good idea to fine-tune as it may lead to overfitting - since the data is similar to the original data, already learned features are very likely to be relevant (\"transferrable\") to this dataset as well, and already generalized from the content in the larger original dataset.\n",
    "\n",
    "If the target dataset is relatively large in size and again of similar content to the original dataset, we can try fine-tuning only as we are less likely to overfit.\n",
    "\n",
    "If our target dataset is smaller in size but dissimilar in content to the original dataset, it is likely that the later domain-specific layers in the original network have learned features that are not relevant to our target network. In this case, it might work better to remove some of these later layers, and add any new domain-specific layers to  activations from earlier in the original network.\n",
    "\n",
    "If our target dataset is large and very dissimilar in content to the original dataset, we could train from scratch, but in practice it is often useful to initialize with weights taken from a pre-trained model. However, we don't need to freeze any of the layers and would typically just use our original dataset as a starting point.\n",
    "\n",
    "Additionally, we will tend to transition from having general (*well-transferrable*) features early in the network, to more specific (*less-transferrable*) features in later layers of the network. This is called *domain discrepancy*. More advanced strategies might take account of this, and could include usung different learning rates by layer - that is, adjusting the learning weight of a layer (determining how *slushy* to make it, from frozen to fully tweakable) depending on its depth in the network.\n",
    "\n",
    "![Transfer Learning Strategies](images/Transfer_Learning_Strategies.png \"Transfer Learning Strategies\")\n",
    "\n",
    "\n",
    "## Our Examples\n",
    "\n",
    "In our examples,  we are going to take a pre-trained set of weights for an object classifier network (ResNet18, trained on ImageNet), and try two different approaches:\n",
    " * In the first approach, we will *freeze* these weights, and only train additional layers to perform semantic segmentation.\n",
    " * In the second approach, we will not freeze the original weights, but *fine-tune* them by continuing to train and allowing backpropagation to alter the weights in the earlier layers of the network.\n",
    " \n",
    "Note that this is not quite the same model that we used in Module 4, Lab 1, as we're using a small model that is pre-trained. As a result, our segmentation accuracy will be more *blocky*, as we are upsampling from smaller layers. But this is an artefact of engineering the lab to run in a limited computation environment, and to keep training size down. In general, transfer learning will offer better accuracy for limited training sets, and quite often faster training as well (if freezing layers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_node_name = \"features\"\n",
    "last_hidden_node_name = \"z.x\"\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "num_channels = 3\n",
    "\n",
    "#\n",
    "# Defines the fully convolutional models for image segmentation (transfer learning)\n",
    "#\n",
    "def create_transfer_learning_model(input, num_classes, model_file, freeze=False):\n",
    "\n",
    "    base_model = load_model(model_file)\n",
    "    base_model = C.as_composite(base_model[3].owner)\n",
    "\n",
    "    # Load the pretrained classification net and find nodes\n",
    "    feature_node = C.logging.find_by_name(base_model, feature_node_name)\n",
    "    last_node = C.logging.find_by_name(base_model, last_hidden_node_name)\n",
    "    \n",
    "    base_model = C.combine([last_node.owner]).clone(C.CloneMethod.freeze if freeze else C.CloneMethod.clone, {feature_node: C.placeholder(name='features')})\n",
    "    base_model = base_model(C.input_variable((num_channels, image_height, image_width)))\n",
    "\n",
    "    r1 = C.logging.find_by_name(base_model, \"z.x.x.r\")\n",
    "    r2_2 = C.logging.find_by_name(base_model, \"z.x.x.x.x.r\")\n",
    "    r3_2 = C.logging.find_by_name(base_model, \"z.x.x.x.x.x.x.r\")\n",
    "\n",
    "    up_r1 = cntk_resnet_fcn.OneByOneConvAndUpSample(r1, 3, num_classes)\n",
    "    up_r2_2 = cntk_resnet_fcn.OneByOneConvAndUpSample(r2_2, 2, num_classes)\n",
    "    up_r3_2 = cntk_resnet_fcn.OneByOneConvAndUpSample(r3_2, 1, num_classes)\n",
    "    \n",
    "    merged = C.splice(up_r1, up_r2_2, up_r3_2, axis=0)\n",
    "\n",
    "    resnet_fcn_out = Convolution((1, 1), num_classes, init=he_normal(), activation=sigmoid, pad=True)(merged)\n",
    "\n",
    "    z = cntk_resnet_fcn.UpSampling2DPower(resnet_fcn_out,2)\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's setup and call our function to setup the large dataset that is  needed for this lab. \n",
    "\n",
    "Why are we doing this? We need this step because Azure Notebooks has two storage areas: a persistent but slow area, and a transient but fast area. To ensure that this lab is able to execute as fast as possible on Azure Notebooks, we unzip our data set into this transient area once per session.  If it has already been unzipped then we'll automatically detect this and skip the step, so it is safe to run this next code block at any time.\n",
    "\n",
    "Depending on how you are executing the lab, this step can take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start unzipping data files in Y:\\Courses\\Computer Vision\\dev290x-1 (1)\\library\\Module4\\../data-zip to Y:\\Courses\\Computer Vision\\dev290x-1 (1)\\library\\Module4\\../../data/M4\n",
      "zip file count:6\n",
      "data folder already populated\n",
      "Complete: Files have been unzipped to Y:\\Courses\\Computer Vision\\dev290x-1 (1)\\library\\Module4\\../../data/M4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import fnmatch\n",
    "\n",
    "def hydrate(zip_path, dest_path):\n",
    "    print(\"Start unzipping data files in {0} to {1}\".format(zip_path,dest_path))\n",
    "    if (os.path.exists(zip_path) == False):\n",
    "        print(\"The source folder {0} doesn't exist, so quitting\".format(zip_path))\n",
    "        quit()\n",
    "\n",
    "    zipfile_count = len(fnmatch.filter(os.listdir(zip_path), '*.zip'))\n",
    "    if (zipfile_count == 0):\n",
    "        print(\"No zip (.zip) files in {0}, so quitting \".format(zip_path))\n",
    "\n",
    "    print(\"zip file count:%s\" % zipfile_count)\n",
    "\n",
    "    if (os.path.exists(dest_path) == False):\n",
    "        print(\"Destination folder {0} doesn't exist, creating it\".format(dest_path))\n",
    "        os.makedirs(dest_path)\n",
    "\n",
    "        # Extract all zip files from zip_path to dest_path\n",
    "        print(\"Start unzipping files to {0}\".format(dest_path))\n",
    "        for item in os.listdir(zip_path): # loop through items in dir\n",
    "            if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "                print(\"   unzipping {0} ...\".format(item))\n",
    "                file_name = os.path.join(zip_path,item) # get full path of files\n",
    "                zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "                zip_ref.extractall(dest_path) # extract file to dir\n",
    "                zip_ref.close() # close file\n",
    "    else:\n",
    "        print(\"data folder already populated\")\n",
    "\n",
    "    print(\"Complete: Files have been unzipped to {0}\".format(dest_path))\n",
    "    \n",
    "hydrate(zip_path, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Configuring data source...\n",
      "Initializing CocoMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[i] reading images and labels...: 100%|####################################################################################################################################| 2986/2986 [00:00<00:00, 47395.06it/s]\n",
      "[i] reading images and labels...: 100%|###################################################################################################################################################| 97/97 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] # training samples:    2986\n",
      "[i] # validation samples:  97\n",
      "[i] # classes:             2\n",
      "[i] Image size:            (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Configure the data source\n",
    "\n",
    "    \n",
    "print('[i] Configuring data source...')\n",
    "try:\n",
    "    source = coco.CocoMs(os.path.join(data_path, \"CocoMS\"))\n",
    "    training_input_image_files, training_target_mask_files = source.get_data(train_data_folder='/Training')\n",
    "    validation_input_image_files, validation_target_mask_files = source.get_data(train_data_folder='/Validation')\n",
    "    print('[i] # training samples:   ', len(training_input_image_files))\n",
    "    print('[i] # validation samples: ', len(validation_input_image_files))\n",
    "    print('[i] # classes:            ', source.num_classes)\n",
    "    print('[i] Image size:           ', (224,224))\n",
    "except (ImportError, AttributeError, RuntimeError) as e:\n",
    "    print('[!] Unable to load data source:', str(e))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to encapsulate our image processing routines from the last lab into a handy function for re-use.\n",
    "\n",
    "This creates some images to visualize how well our semantic segmenter worked out against our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drawing\n",
    "\n",
    "def process_images():\n",
    "    print(\"[i] Started image processing...\", flush=True)\n",
    "    tic = time.time()\n",
    "    \n",
    "    input_images_rgb = []\n",
    "    for x in tqdm(validation_input_images, ascii=True, desc='[i] Converting input images (BGR2RGB)...'):\n",
    "        img = np.moveaxis(x,0,2).astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        input_images_rgb.append(img)\n",
    "\n",
    "    target_masks_rgb=[]\n",
    "    for x in tqdm(validation_target_masks, ascii=True, desc='[i] Coloring ground truth images...'):\n",
    "        target_masks_rgb.append(helper.masks_to_colorimg(x))\n",
    "\n",
    "    pred_rgb=[]\n",
    "    for x in tqdm(pred, ascii=True, desc='[i] Coloring prediction images...'):\n",
    "        pred_rgb.append(helper.masks_to_colorimg(x))\n",
    "\n",
    "    output_images_rgb = []\n",
    "    for index in tqdm(range(len(input_images_rgb)), ascii=True, desc='[i] Combining input images + predictions...'):\n",
    "        img = cv2.bitwise_or(input_images_rgb[index], pred_rgb[index])\n",
    "        output_images_rgb.append(img)\n",
    "\n",
    "    print('Image Processing time: {} s.'.format(time.time() - tic))\n",
    "    print(\"Image processing finished... Now plotting (this can take a while - 2-3 minutes on Azure Notebooks)...\", flush=True)\n",
    "    helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb, output_images_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Trainer\n",
    "\n",
    "This time, our training function was create transfer learning models for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_image_files, train_mask_files, val_image_files, val_mask_files, base_model_file, freeze=False):\n",
    "    # Create model\n",
    "    sample_img, sample_mask = source.files_to_data([train_image_files[0]], [val_image_files[0]])\n",
    "    x = C.input_variable(sample_img[0].shape)\n",
    "    y = C.input_variable(sample_mask[0].shape)\n",
    "    \n",
    "    z = create_transfer_learning_model(x, source.num_classes, base_model_file, freeze)\n",
    "    dice_coef = cntk_resnet_fcn.dice_coefficient(z, y)\n",
    "\n",
    "\n",
    "    # Prepare model and trainer\n",
    "    if (isUsingGPU):\n",
    "        lr_mb = [0.001] * 5 + [0.0001] * 5 + [0.00001]*5 + [0.000001]*5 + [0.0000001]*5\n",
    "    else:\n",
    "        # training without a CPU is really slow, so we'll deliberatly shrink the amount of training\n",
    "        # to just an epoch if we're on a CPU - just to give a flavor of what happens during training\n",
    "        # and then read in a pre-trained model for inference instead.\n",
    "        lr_mb = [0.0001] * 1 # deliberately shrink if training on CPU...\n",
    "    lr = learning_rate_schedule(lr_mb, UnitType.sample)\n",
    "    momentum = C.learners.momentum_as_time_constant_schedule(0.9)\n",
    "    trainer = C.Trainer(z, (-dice_coef, -dice_coef), C.learners.adam(z.parameters, lr=lr, momentum=momentum))\n",
    "                        \n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    # Get minibatches of training data and perform model training\n",
    "    minibatch_size = 8\n",
    "    num_epochs = len(lr_mb)\n",
    "     \n",
    "    for e in range(0, num_epochs):\n",
    "        for i in tqdm(range(0, int(len(train_image_files) / minibatch_size)), ascii=True, \n",
    "                               desc=\"[i] Processing epoch {}/{}\".format(e, num_epochs-1)):\n",
    "            data_x_files, data_y_files = training_helper.slice_minibatch(train_image_files, train_mask_files, i, minibatch_size)\n",
    "            data_x, data_y = source.files_to_data(data_x_files, data_y_files)\n",
    "            trainer.train_minibatch({z.arguments[0]: data_x, y: data_y})\n",
    "            gc.collect()\n",
    "     \n",
    "        # Measure training error\n",
    "        training_error = training_helper.measure_error(source, data_x_files, data_y_files, z.arguments[0], y, trainer, minibatch_size)\n",
    "        training_errors.append(training_error)\n",
    "        \n",
    "        # Measure test error\n",
    "        test_error = training_helper.measure_error(source, val_image_files, val_mask_files, z.arguments[0], y, trainer, minibatch_size)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        print(\"epoch #{}: training_error={}, test_error={}\".format(e, training_errors[-1], test_errors[-1]))\n",
    "        \n",
    "    return trainer, training_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing\n",
    "\n",
    "And now time to start training.  In the first approach, we will *freeze* these weights, and only train additional layers to perform semantic segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Converting file lists to image data...\n",
      "Converting validation image data time: 3.605370044708252 s.\n",
      "[i] Converting file lists finished...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We need to convert our validation filenames to image data for the predictor...\n",
    "#\n",
    "\n",
    "print (\"[i] Converting file lists to image data...\", flush=True)\n",
    "tic = time.time()\n",
    "validation_input_images, validation_target_masks = \\\n",
    "        source.files_to_data(validation_input_image_files, validation_target_mask_files)\n",
    "print('Converting validation image data time: {} s.'.format(time.time() - tic))\n",
    "print(\"[i] Converting file lists finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Skipping training, using pre-trained model...\n",
      "[i] Starting prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[i] Predicting...: 100%|##########################################################################################################################################################| 97/97 [00:17<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] 97 images predicted.\n",
      "[i] Prediction finished...\n"
     ]
    }
   ],
   "source": [
    "# Training with original layer weights frozen)\n",
    "\n",
    "if make_model:\n",
    "    print(\"[i] Starting training (with frozen weights)...\", flush=True)\n",
    "    frozen = True\n",
    "    tic = time.time()\n",
    "    trainer, training_errors, test_errors = train(training_input_image_files, training_target_mask_files,  \n",
    "                                                  validation_input_image_files, validation_target_mask_files, \n",
    "                                                  base_model_file, frozen)\n",
    "    print('Training time: {}'.format(time.time() - tic))\n",
    "    print(\"[i] Training finished...\")\n",
    "    model = trainer.model\n",
    "else:\n",
    "    print(\"[i] Skipping training, using pre-trained model...\")\n",
    "    model = C.load_model(os.path.join(model_path, 'cntk-resnet-fcn-transfer-frozen.dnn'))\n",
    "\n",
    "# Prediction\n",
    "\n",
    "print(\"[i] Starting prediction...\", flush=True)\n",
    "\n",
    "pred = []\n",
    "for idx in tqdm(range(0, len(validation_input_images)),ascii=True, desc='[i] Predicting...'):\n",
    "    pred += list(model.eval(validation_input_images[idx]))\n",
    "\n",
    "print('[i] {} images predicted.'.format(len(pred)))\n",
    "print(\"[i] Prediction finished...\")\n",
    "\n",
    "if make_model:\n",
    "    helper.plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Started image processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[i] Converting input images (BGR2RGB)...: 100%|##################################################################################################################################| 97/97 [00:00<00:00, 404.35it/s]\n",
      "[i] Coloring ground truth images...: 100%|#######################################################################################################################################| 97/97 [00:00<00:00, 325.07it/s]\n",
      "[i] Coloring prediction images...: 100%|#########################################################################################################################################| 97/97 [00:00<00:00, 592.86it/s]\n",
      "[i] Combining input images + predictions...: 100%|##############################################################################################################################| 97/97 [00:00<00:00, 1005.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Processing time: 0.7565522193908691 s.\n",
      "Image processing finished... Now plotting (this can take a while - 2-3 minutes on Azure Notebooks)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################################################################################################################| 388/388 [00:15<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collection reclaimed 0 objects\n"
     ]
    }
   ],
   "source": [
    "process_images()\n",
    "print(\"Garbage collection reclaimed {} objects\".format(gc.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice how much faster training is with transfer learning and frozen initial layers versus the full model training we did in Module 4 Lab 1? We're just training a few additional layers to implement Semantic Segmentation based on a base model of ResNet...\n",
    "\n",
    "For the frozen layers, we don't have to retrain them -- their weights are frozen. What this means is that we don't have to run back prop on them to calculate error gradients and update weights. We just do forward prop through those layers and back prop only on the final additional layers to train their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning\n",
    "\n",
    "In the second approach, we will not freeze the original weights, but *fine-tune* them by continuing to train and allowing backpropagation to alter the weights in the earlier layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Starting prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[i] Predicting...: 100%|##########################################################################################################################################################| 97/97 [00:10<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] 97 images predicted.\n",
      "[i] Prediction finished...\n"
     ]
    }
   ],
   "source": [
    "# Training with Fine-Tuning\n",
    "\n",
    "if make_model:\n",
    "    print(\"[i] Starting training (with fine-tuning)...\", flush=True)\n",
    "    frozen = False\n",
    "    tic = time.time()\n",
    "    trainer, training_errors, test_errors = train(training_input_image_files, training_target_mask_files, \n",
    "                                                  validation_input_image_files, validation_target_mask_files, \n",
    "                                                  base_model_file, frozen)\n",
    "    print('Training time: {} s.'.format(time.time() - tic))\n",
    "    print(\"[i] Training finished...\")\n",
    "    model = trainer.model\n",
    "else:\n",
    "    model = C.load_model(os.path.join(model_path, 'cntk-resnet-fcn-transfer-finetune.dnn'))\n",
    "\n",
    "# Prediction\n",
    "\n",
    "print(\"[i] Starting prediction...\", flush=True)\n",
    "\n",
    "pred = []\n",
    "for idx in tqdm(range(0, len(validation_input_images)),ascii=True, desc='[i] Predicting...'):\n",
    "    pred += list(model.eval(validation_input_images[idx]))\n",
    "\n",
    "print('[i] {} images predicted.'.format(len(pred)))\n",
    "print(\"[i] Prediction finished...\")\n",
    "\n",
    "if make_model:\n",
    "    helper.plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")\n",
    "    # clean-up some variables we no longer need to reduce our memory footprint...\n",
    "    # otherwise our Azure Notebook might run out of memory\n",
    "    del trainer\n",
    "    del training_errors\n",
    "    del test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Started image processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[i] Converting input images (BGR2RGB)...: 100%|#################################################################################################################################| 97/97 [00:00<00:00, 1593.61it/s]\n",
      "[i] Coloring ground truth images...: 100%|#######################################################################################################################################| 97/97 [00:00<00:00, 679.91it/s]\n",
      "[i] Coloring prediction images...: 100%|#########################################################################################################################################| 97/97 [00:00<00:00, 698.47it/s]\n",
      "[i] Combining input images + predictions...: 100%|##############################################################################################################################| 97/97 [00:00<00:00, 8112.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Processing time: 0.3802478313446045 s.\n",
      "Image processing finished... Now plotting (this can take a while - 2-3 minutes on Azure Notebooks)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################################################################################################################| 388/388 [00:14<00:00, 26.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collection reclaimed 0 objects\n"
     ]
    }
   ],
   "source": [
    "process_images()\n",
    "print(\"Garbage collection reclaimed {} objects\".format(gc.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this lab, we added fully-convolutional layers to a base ResNet model and use the concept of Transfer Learning to train our models.\n",
    "\n",
    "Transfer Learning is the defacto means of training models nowadays for new data sets. It is a technique that allows a network to learn a new skill (recognition of a new class of object, for instance) by employing the knowledge it has already learned about similar types of skills (i.e., an ability to detect edges, corners, and more complex shapes, built up in its various layers through learning to recognise difference classes of objects with a large data set).\n",
    "\n",
    "We too a pre-trained set of weights for an object classifier network (ResNet, trained on ImageNet), and tried two different approaches - the first where we froze the base model weights, and only allowed the new layers to learn, and the second where we seeded the initial layers with weights from the base model, but allowed all layers in the model to update and learn a solution to our problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
